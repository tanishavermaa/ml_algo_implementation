{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. The mathematical formula for a linear Support Vector Machine (SVM) can be represented as follows:\n",
    "\n",
    "Given a dataset of N data points with features xᵢ (i = 1 to N) and corresponding labels yᵢ (yᵢ = ±1 for binary classification), the linear SVM aims to find a hyperplane that maximizes the margin between the two classes while minimizing classification errors. The equation of this hyperplane can be represented as:\n",
    "\n",
    "w^T * x + b = 0\n",
    "\n",
    "Where:\n",
    "\n",
    "w is the weight vector that defines the orientation of the hyperplane.\n",
    "b is the bias or intercept term.\n",
    "x represents the feature vector.\n",
    "The decision boundary is where w^T * x + b = 0. The goal is to find the optimal w and b that best separate the data points into two classes.\n",
    "\n",
    "Q2. The objective function of a linear SVM is to maximize the margin while minimizing classification errors. Mathematically, it can be expressed as:\n",
    "\n",
    "Minimize 1/2 * ||w||² subject to yᵢ * (w^T * xᵢ + b) ≥ 1 for all data points (for linearly separable data)\n",
    "\n",
    "In this objective function:\n",
    "\n",
    "\"w\" is the weight vector.\n",
    "\"b\" is the bias term.\n",
    "\"xᵢ\" is a data point.\n",
    "\"yᵢ\" is the corresponding class label.\n",
    "The goal is to find the values of \"w\" and \"b\" that satisfy the constraints while maximizing the margin between the two classes. The margin is defined as 2/||w||.\n",
    "\n",
    "Q3. The kernel trick in SVM is a technique used to transform data into a higher-dimensional feature space to make it linearly separable when it is not in the original feature space. It involves replacing the inner product (dot product) of the data points with a kernel function. The kernel function is a function that computes the similarity between data points in the higher-dimensional space.\n",
    "\n",
    "Common kernel functions include:\n",
    "\n",
    "Linear Kernel: K(xᵢ, xⱼ) = xᵢ^T * xⱼ\n",
    "Polynomial Kernel: K(xᵢ, xⱼ) = (γ * xᵢ^T * xⱼ + r)ᵒ\n",
    "Radial Basis Function (RBF) Kernel: K(xᵢ, xⱼ) = exp(-γ * ||xᵢ - xⱼ||²)\n",
    "The kernel trick allows SVM to find a hyperplane in this higher-dimensional space, even when the data is not linearly separable in the original space.\n",
    "\n",
    "Q4. The support vectors in SVM are the data points that are closest to the hyperplane and play a crucial role in defining the decision boundary. Support vectors are the points for which the margin is exactly 1. The main roles of support vectors are:\n",
    "\n",
    "They help define the position and orientation of the decision boundary or hyperplane.\n",
    "They have non-zero Lagrange multipliers (αᵢ) in the SVM dual problem, indicating their importance in the final solution.\n",
    "They represent the most challenging data points that are closest to the decision boundary.\n",
    "Example: Suppose you have a binary classification problem with two classes (A and B), and the data points are not linearly separable. The support vectors are the data points from class A and class B that are nearest to the decision boundary. These support vectors define the margin and contribute to finding the optimal hyperplane.\n",
    "\n",
    "Q5. Illustrating Hyperplane, Marginal plane, Soft margin, and Hard margin in SVM:\n",
    "\n",
    "Hyperplane: The hyperplane is the decision boundary that separates two classes in a linear SVM. It is represented by the equation w^T * x + b = 0.\n",
    "\n",
    "Marginal Plane: The marginal plane is a hyperplane that is equidistant from the support vectors of both classes. It defines the margin, which is the distance between the marginal plane and the support vectors.\n",
    "\n",
    "Soft Margin: In a soft-margin SVM, some misclassification of data points is allowed to handle noisy or overlapping data. It introduces a parameter \"C\" to control the trade-off between maximizing the margin and minimizing classification errors. A smaller \"C\" allows for a wider margin but tolerates more misclassifications.\n",
    "\n",
    "Hard Margin: In a hard-margin SVM, no misclassification is allowed. It requires that all data points are correctly classified. This is suitable for linearly separable data but may not work for real-world datasets with noise or overlapping classes.\n",
    "\n",
    "\n",
    "\n",
    "Q6. Polynomial functions and kernel functions in machine learning are related in the context of kernel methods, including Support Vector Machines (SVM). Polynomial functions can be used as kernel functions to transform data into a higher-dimensional space, making it linearly separable. The relationship can be summarized as follows:\n",
    "\n",
    "Polynomial Kernel: The polynomial kernel is a specific kernel function used in SVM and other kernel methods. It takes the form K(xᵢ, xⱼ) = (γ * xᵢ^T * xⱼ + r)ᵒ, where γ, r, and ᵒ are kernel parameters.\n",
    "\n",
    "Kernel Trick: The kernel trick is a technique that allows SVM to operate in this higher-dimensional space without explicitly calculating the transformation of data points. It replaces the inner product of data points in the higher-dimensional space with the result of the kernel function, K(xᵢ, xⱼ).\n",
    "\n",
    "Relationship: When you choose a polynomial kernel with a suitable degree (ᵒ), it can perform a polynomial transformation on the data. For example, a polynomial kernel with degree 2 can transform the data into a quadratic feature space, making it possible to find a quadratic decision boundary in the original feature space. The choice of γ and r in the kernel function further influences the behavior of the kernel.\n",
    "\n",
    "In summary, polynomial kernels are a specific type of kernel function that can be used with SVM to transform data into higher-dimensional spaces, making them suitable for handling non-linearly separable data.\n",
    "\n",
    "Q8. In Support Vector Regression (SVR), increasing the value of epsilon (ε) does not directly affect the number of support vectors. The number of support vectors depends on the data distribution and the choice of the kernel function, not on the value of epsilon. Epsilon in SVR controls the width of the ε-insensitive tube, which determines the level of tolerance for errors in the prediction.\n",
    "\n",
    "Epsilon defines a range within which errors are considered negligible and do not contribute to the loss function. Points outside this range are penalized based on their distance from the predicted values.\n",
    "\n",
    "The number of support vectors is determined by the complexity of the problem, the choice of the kernel function, and the value of the C parameter, which balances the trade-off between fitting the training data and regularization. A higher value of C may lead to more support vectors, indicating a more complex model that fits the training data closely.\n",
    "\n",
    "Q9. In Support Vector Regression (SVR), the choice of kernel function, C parameter, epsilon parameter, and gamma parameter can significantly affect the model's performance:\n",
    "\n",
    "Kernel Function: The kernel function determines how data is transformed into a higher-dimensional space. For non-linear data, choosing the appropriate kernel function is crucial. Common choices include linear, polynomial, and radial basis function (RBF) kernels. The choice of the kernel should be guided by the data's characteristics.\n",
    "\n",
    "C Parameter: The C parameter controls the trade-off between fitting the training data and regularization. A smaller C value allows for a wider margin and can tolerate more errors on the training data. A larger C value leads to a narrower margin and a more accurate fit to the training data. The choice of C depends on the problem's tolerance for errors and the dataset's noise.\n",
    "\n",
    "Epsilon Parameter: The epsilon (ε) parameter defines the width of the ε-insensitive tube. A smaller ε means that the model is less tolerant of errors, while a larger ε allows for more errors within the tolerance range. The choice of ε depends on the desired level of flexibility in the model and the noise in the data.\n",
    "\n",
    "Gamma Parameter: The gamma (γ) parameter is specific to some kernel functions, such as the RBF kernel. It controls the shape and scale of the kernel function. A smaller γ results in a wider, smoother kernel, while a larger γ leads to a narrower, more localized kernel. The choice of γ should be based on the data's characteristics, and tuning it correctly is essential for model performance.\n",
    "\n",
    "The impact of these parameters on performance is problem-specific, and they often require tuning through techniques like cross-validation to find the best combination for your specific regression problem.\n",
    "\n",
    "Q10. If your goal is to predict the actual price of a house as accurately as possible, using Mean Squared Error (MSE) as the evaluation metric would be more appropriate. MSE measures the average squared difference between the predicted values and the actual prices. Minimizing MSE results in the model providing predictions that are as close as possible to the actual prices, making it a suitable choice for regression tasks like predicting house prices.\n",
    "\n",
    "R-squared (R²) measures the proportion of the variance in the dependent variable that is explained by the independent variables. While R-squared can be useful for understanding how well your features explain the variance, it may not be the best choice if the primary goal is to minimize prediction errors, as MSE directly quantifies prediction accuracy.\n",
    "\n",
    "Q11. In a dataset with a significant number of outliers, Mean Absolute Error (MAE) would be the most appropriate regression metric to use with your SVM model. MAE is less sensitive to outliers compared to Mean Squared Error (MSE) and Root Mean Squared Error (RMSE).\n",
    "\n",
    "MAE calculates the average absolute difference between the predicted values and the actual target values. Outliers can have a substantial impact on squared error metrics like MSE and RMSE because they square the differences. MAE, on the other hand, treats all errors equally and provides a robust measure of model performance in the presence of outliers.\n",
    "\n",
    "Q12. If both the Mean Squared Error (MSE) and Root Mean Squared Error (RMSE) values are very close, it is typically recommended to choose RMSE as the evaluation metric for your SVM regression model. RMSE has the advantage of having the same units as the target variable, making it more interpretable. It represents the standard deviation of the model's prediction errors and is commonly used in regression tasks for practical understanding.\n",
    "\n",
    "While MSE and RMSE provide similar information, RMSE is preferred in scenarios where you want to express the model's performance in the same units as the target variable, allowing for easier interpretation of prediction errors.\n",
    "\n",
    "Q13. When comparing the performance of different SVM regression models using different kernels (linear, polynomial, and RBF), and the goal is to measure how well the model explains the variance in the target variable, the most appropriate metric is the coefficient of determination, often denoted as R-squared (R²).\n",
    "\n",
    "R-squared measures the proportion of the variance in the target variable that is explained by the model. A higher R² value indicates that the model accounts for more variance in the data. It is a suitable metric for assessing how well each model captures the variation in the target variable, making it a good choice for this scenario.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import load_npz\n",
    "X_y=load_npz(\"class_X_y.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X_y[:,:-1]\n",
    "y=X_y[:,-1]\n",
    "X=X.toarray()\n",
    "y=y.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.33,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc=SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\eda_fe_regress&class\\env\\lib\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05519804,  0.00593964, -0.00305198, ...,  0.00277915,\n",
       "         0.02077447,  0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      1024\n",
      "           1       0.94      0.88      0.91       161\n",
      "\n",
      "    accuracy                           0.98      1185\n",
      "   macro avg       0.96      0.94      0.95      1185\n",
      "weighted avg       0.98      0.98      0.98      1185\n",
      "\n",
      "[[1015    9]\n",
      " [  19  142]]\n",
      "0.9763713080168777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning with SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameter={\n",
    "    'C':[0.1],\n",
    "    'gamma':[1,0.1],\n",
    "    'kernel':['linear']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid=GridSearchCV(SVC(),param_grid=parameter,cv=2,verbose=3)\n",
    "y_train = y_train.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
      "[CV 1/2] END .....C=0.1, gamma=1, kernel=linear;, score=0.956 total time= 2.3min\n",
      "[CV 2/2] END .....C=0.1, gamma=1, kernel=linear;, score=0.950 total time= 1.6min\n",
      "[CV 1/2] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.956 total time=  57.6s\n",
      "[CV 2/2] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.950 total time= 2.0min\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=2, estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: [0.1], &#x27;gamma&#x27;: [1, 0.1], &#x27;kernel&#x27;: [&#x27;linear&#x27;]},\n",
       "             verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=2, estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: [0.1], &#x27;gamma&#x27;: [1, 0.1], &#x27;kernel&#x27;: [&#x27;linear&#x27;]},\n",
       "             verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=2, estimator=SVC(),\n",
       "             param_grid={'C': [0.1], 'gamma': [1, 0.1], 'kernel': ['linear']},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.1, 'gamma': 1, 'kernel': 'linear'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      1024\n",
      "           1       0.96      0.84      0.90       161\n",
      "\n",
      "    accuracy                           0.97      1185\n",
      "   macro avg       0.97      0.92      0.94      1185\n",
      "weighted avg       0.97      0.97      0.97      1185\n",
      "\n",
      "[[1018    6]\n",
      " [  25  136]]\n",
      "0.9738396624472574\n"
     ]
    }
   ],
   "source": [
    "y_pred=grid.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
